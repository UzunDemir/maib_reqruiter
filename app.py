import os
import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import json
from PyPDF2 import PdfReader
import tempfile
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import docx
from docx import Document
import io

from concurrent.futures import ThreadPoolExecutor

st.set_page_config(layout="wide", initial_sidebar_state="expanded")

# --- Stiluri È™i bara lateralÄƒ ---
st.markdown("""
    <style>

        section[data-testid="stSidebar"] {
            background-color: #253646 !important;
        }
        .sidebar-title {
            color: white;
            font-size: 24px;
            font-weight: bold;
            text-align: center;
            margin-bottom: 1rem;
        }
        .sidebar-text {
            color: white;
        }
        #MainMenu, footer, header {
            display: none !important;
        }
        .center {
            display: flex;
            justify-content: center;
            align-items: center;
            text-align: center;
            flex-direction: column;
            margin-top: 0vh;
        }
        .match-card {
            border-radius: 10px;
            padding: 15px;
            margin-bottom: 15px;
            background-color: #f0f2f6;
        }
        .match-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        .progress-bar {
            height: 10px;
            background-color: #e0e0e0;
            border-radius: 5px;
            margin-top: 5px;
        }
        .progress-fill {
            height: 100%;
            border-radius: 5px;
            background-color: #40c1ac;
        }
    </style>
    <div class="center">
        <img src="https://www.maib.md/uploads/custom_blocks/image_1633004921_8nR1jw3Qfu_auto__0.png" width="300">
        <h1>AI HR-Recruiter</h1>
    </div>
""", unsafe_allow_html=True)

import streamlit as st

st.sidebar.markdown('<div class="sidebar-title">MAIB AI HR-Recruiter</div>', unsafe_allow_html=True)
st.sidebar.divider()
st.sidebar.markdown("""
<div class="sidebar-text">

1. ğŸ“¥ **ÃncÄƒrcarea posturilor vacante**  

   *Agentul Ã®ncarcÄƒ automat toate posturile vacante actuale de la MAIB.*

2. ğŸ“„ **CV-ul utilizatorului**  

   *Utilizatorul Ã®È™i Ã®ncarcÄƒ CV-ul pentru analizÄƒ.*

3. ğŸ¤– **CÄƒutarea posturilor potrivite**  

   *Agentul analizeazÄƒ CV-ul È™i identificÄƒ **top 3 posturi** relevante pentru experienÈ›a È™i competenÈ›ele candidatului.*

4. ğŸ” **Analiza celei mai relevante poziÈ›ii**  

   * *EvidenÈ›iazÄƒ **punctele forte** ale candidatului.*  
   * *IdentificÄƒ **punctele slabe** sau lipsurile Ã®n competenÈ›e.*

5. âœ… **Acordul candidatului**  

   *DacÄƒ este interesat, candidatul Ã®È™i exprimÄƒ acordul pentru a continua procesul.*

6. ğŸ—£ï¸ **Primul interviu (general)**  

   *Agentul pune Ã®ntrebÄƒri generale, analizeazÄƒ rÄƒspunsurile È™i formuleazÄƒ **primele concluzii**.*

7. ğŸ’» **Interviul tehnic**  

   *Evaluarea competenÈ›elor tehnice È™i furnizarea unui **feedback tehnic**.*

8. ğŸ“‹ **Concluzia finalÄƒ**  

   *Agentul oferÄƒ un verdict final: **recomandare pentru angajare** sau **refuz argumentat**.*

</div>
""", unsafe_allow_html=True)

st.divider()


class DocumentChunk:
    def __init__(self, text, doc_name, page_num):
        self.text = text
        self.doc_name = doc_name
        self.page_num = page_num

class KnowledgeBase:
    def __init__(self):
        self.chunks = []
        self.uploaded_files = []
        self.doc_texts = []
    
    def clear(self):
        self.chunks = []
        self.doc_texts = []
        self.uploaded_files = []
    
    def split_text(self, text, max_chars=2000):
        chunks = []
        start = 0
        while start < len(text):
            end = min(start + max_chars, len(text))
            chunk = text[start:end].strip()
            if chunk:  # Skip empty chunks
                chunks.append(chunk)
            start = end
        return chunks

    def load_pdf(self, file_content, file_name):
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(file_content)
                tmp_file_path = tmp_file.name

            with open(tmp_file_path, 'rb') as file:
                reader = PdfReader(file)
                for page_num, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    if page_text:
                        for chunk in self.split_text(page_text):
                            self.chunks.append(DocumentChunk(chunk, file_name, page_num+1))
                            self.doc_texts.append(chunk)
            self.uploaded_files.append(file_name)
            return True
        except Exception as e:
            st.error(f"Eroare la Ã®ncÄƒrcarea PDF: {str(e)}")
            return False
        finally:
            if os.path.exists(tmp_file_path):
                os.unlink(tmp_file_path)

    def load_docx(self, file_content, file_name):
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp_file:
                tmp_file.write(file_content)
                tmp_file_path = tmp_file.name

            doc = docx.Document(tmp_file_path)
            full_text = []
            for para in doc.paragraphs:
                text = para.text.strip()
                if text:  # Skip empty paragraphs
                    full_text.append(text)
            text = "\n".join(full_text)
            for chunk in self.split_text(text):
                self.chunks.append(DocumentChunk(chunk, file_name, 0))
                self.doc_texts.append(chunk)
            self.uploaded_files.append(file_name)
            return True
        except Exception as e:
            st.error(f"Eroare la Ã®ncÄƒrcarea DOCX: {str(e)}")
            return False
        finally:
            if os.path.exists(tmp_file_path):
                os.unlink(tmp_file_path)

    def load_txt(self, file_content, file_name):
        try:
            text = file_content.decode('utf-8', errors='ignore')
            for chunk in self.split_text(text):
                self.chunks.append(DocumentChunk(chunk, file_name, 0))
                self.doc_texts.append(chunk)
            self.uploaded_files.append(file_name)
            return True
        except Exception as e:
            st.error(f"Eroare la Ã®ncÄƒrcarea TXT: {str(e)}")
            return False

    def load_file(self, uploaded_file):
        name = uploaded_file.name.lower()
        content = uploaded_file.read()
        if name.endswith('.pdf'):
            return self.load_pdf(content, uploaded_file.name)
        elif name.endswith('.docx'):
            return self.load_docx(content, uploaded_file.name)
        elif name.endswith('.txt'):
            return self.load_txt(content, uploaded_file.name)
        else:
            st.warning(f"Formatul fiÈ™ierului {uploaded_file.name} nu este suportat.")
            return False

    def get_all_text(self):
        return "\n\n".join(self.doc_texts)




# IniÈ›ializare baza de cunoÈ™tinÈ›e
if 'knowledge_base' not in st.session_state:
    st.session_state.knowledge_base = KnowledgeBase()

if 'vacancies_data' not in st.session_state:
    st.session_state.vacancies_data = []

##############################
# ÃncÄƒrcare oferte de pe rabota.md
def scrape_vacancy(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
        soup_vac = BeautifulSoup(resp.text, 'html.parser')

        title_tag = soup_vac.find('h1')
        title = title_tag.get_text(strip=True) if title_tag else 'Titlu indisponibil'

        vacancy_content = soup_vac.find('div', class_='vacancy-content')
        description = vacancy_content.get_text(separator='\n', strip=True) if vacancy_content else 'Descriere indisponibilÄƒ'

        return {'url': url, 'title': title, 'description': description}
    except Exception as e:
        st.error(f"Eroare la preluarea ofertei {url}: {str(e)}")
        return None

#######################################################################################################################
from concurrent.futures import ThreadPoolExecutor, as_completed

def load_vacancies():
    base_url = "https://www.rabota.md/ru/companies/moldova-agroindbank#vacancies"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    with st.spinner("Se Ã®ncarcÄƒ ofertele de muncÄƒ..."):
        try:
            response = requests.get(base_url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            links = soup.find_all('a', class_='vacancyShowPopup')
            urls = [urljoin(base_url, a['href']) for a in links]

            vacancies_data = []
            progress_bar = st.progress(0)
            status_text = st.empty()

            total = len(urls)
            done = 0

            with ThreadPoolExecutor(max_workers=5) as executor:
                future_to_url = {executor.submit(scrape_vacancy, url): url for url in urls}
                
                for future in as_completed(future_to_url):
                    result = future.result()
                    if result:
                        vacancies_data.append(result)
                        done += 1
                        progress_bar.progress(done / total)
                        status_text.text(f"[{done}/{total}] OfertÄƒ Ã®ncÄƒrcatÄƒ: {result['title']}")

            st.session_state.vacancies_data = vacancies_data
            st.success(f"Au fost Ã®ncÄƒrcate {len(vacancies_data)} oferte de muncÄƒ!")

        except Exception as e:
            st.error(f"Eroare la Ã®ncÄƒrcarea ofertelor: {str(e)}")
##########################################################################################################################################





if st.button("ÃncarcÄƒ ofertele de muncÄƒ pentru tine..."):
    load_vacancies()

# AfiÈ™are lista oferte Ã®n bara lateralÄƒ
if st.session_state.vacancies_data:
    with st.sidebar:
        st.markdown("### ğŸ” Lista ofertelor MAIB:")
        st.success(f"Oferte gÄƒsite: {len(st.session_state.vacancies_data)}")
        for vac in st.session_state.vacancies_data:
            st.markdown(
                f'<a href="{vac["url"]}" target="_blank" style="color:#40c1ac; text-decoration:none;">â€¢ {vac["title"]}</a>',
                unsafe_allow_html=True
            )

##############################
# ÃncÄƒrcare CV
st.markdown("### ğŸ“„ ÃncÄƒrcÄƒ CV-ul tÄƒu (PDF, DOCX, TXT)")
uploaded_files = st.file_uploader("SelecteazÄƒ fiÈ™ier(e)", type=['pdf', 'docx', 'txt'], accept_multiple_files=True)

if uploaded_files:
    kb = st.session_state.knowledge_base
    kb.clear()  # CurÄƒÈ›Äƒ conÈ›inutul anterior
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, uploaded_file in enumerate(uploaded_files):
        success = kb.load_file(uploaded_file)
        progress = (i + 1) / len(uploaded_files)
        progress_bar.progress(progress)
        status_text.text(f"Se proceseazÄƒ fiÈ™ierul {i+1}/{len(uploaded_files)}: {uploaded_file.name}")
    
    st.session_state.knowledge_base = kb
    st.success(f"Au fost Ã®ncÄƒrcate {len(uploaded_files)} fiÈ™iere!")

if not st.session_state.knowledge_base.uploaded_files:
    st.info("Te rugÄƒm sÄƒ Ã®ncarci un CV pentru analizÄƒ")
    st.stop()

##############################
# Analiza potrivirilor
st.markdown("### ğŸ” Cele mai relevante oferte pentru CV-ul tÄƒu")

cv_text = st.session_state.knowledge_base.get_all_text()
vacancies = st.session_state.vacancies_data

if not vacancies:
    st.warning("Nu existÄƒ oferte de muncÄƒ disponibile. Te rugÄƒm sÄƒ Ã®ncarci ofertele mai Ã®ntÃ¢i.")
    st.stop()

# Procesare avansatÄƒ a textelor
vacancy_texts = [f"{vac['title']}\n{vac['description']}" for vac in vacancies]
documents = [cv_text] + vacancy_texts

# TF-IDF Ã®mbunÄƒtÄƒÈ›it
vectorizer = TfidfVectorizer(
    stop_words=None,
    ngram_range=(1, 2),  # Include bigrame pentru mai mult context
    max_features=5000
)
##############################################################################


import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import streamlit as st
import requests
import io
from docx import Document  # pip install python-docx



# --- ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹ ---
try:
    with st.spinner("Se analizeazÄƒ potrivirile..."):
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(documents)
        similarity_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()

    score_min, score_max = similarity_scores.min(), similarity_scores.max()
    if score_max - score_min > 0:
        normalized_scores = (similarity_scores - score_min) / (score_max - score_min) * 100
    else:
        normalized_scores = np.zeros_like(similarity_scores)

    normalized_scores = np.clip(normalized_scores, 0, 100)

    top_indices = similarity_scores.argsort()[::-1][:3]

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ top_indices Ğ² ÑĞµÑÑĞ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ»ÑŒÑˆĞµ
    st.session_state.top_indices = top_indices

    for idx in top_indices:
        vac = vacancies[idx]
        score = normalized_scores[idx]

        with st.container():
            st.markdown(f"""
            <div class="match-card">
                <div class="match-header">
                    <h3><a href="{vac['url']}" target="_blank" style="text-decoration:none; color:inherit;">{vac['title']}</a></h3>
                    <h4>{score:.0f}% potrivire</h4>
                </div>
                <div class="progress-bar">
                    <div class="progress-fill" style="width: {score}%"></div>
                </div>
            </div>
            """, unsafe_allow_html=True)

        st.write("---")

except Exception as e:
    st.error(f"Eroare la analiza potrivirilor: {str(e)}")

# --- Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ Deepseek API ---
api_key = st.secrets.get("DEEPSEEK_API_KEY")
if not api_key:
    st.error("API ĞºĞ»ÑÑ‡ Ğ½Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½. ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ´Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ ĞµĞ³Ğ¾ Ğ² Secrets.")
    st.stop()

url = "https://api.deepseek.com/v1/chat/completions"
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json"
}

# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ top_indices Ğ¸Ğ· ÑĞµÑÑĞ¸Ğ¸, ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
top_indices = st.session_state.get("top_indices", [])
if len(top_indices) > 0:
    best_match_idx = top_indices[0]
    best_match_vacancy = vacancies[best_match_idx]

    # ĞÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµĞ¼ ĞºĞ½Ğ¾Ğ¿ĞºÑƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
    if st.button("ğŸ” GenereazÄƒ analiza de potrivire"):
        prompt = f"""
        AnalizeazÄƒ corespondenÈ›a dintre CV-ul candidatului È™i oferta de muncÄƒ.
        Mai Ã®ntÃ¢i voi furniza CV-ul, apoi descrierea postului.

        CV-ul candidatului:
        {cv_text}
        
        Descrierea postului:
        {best_match_vacancy['description']}
        
        VÄƒ rog sÄƒ efectuaÈ›i analiza conform urmÄƒtoarei structuri:

        1. Punctele forte ale CV-ului (potrivirea exactÄƒ cu cerinÈ›ele postului)
        2. Punctele slabe sau lacunele din CV (unde candidatul nu corespunde)
        3. RecomandÄƒri concrete pentru Ã®mbunÄƒtÄƒÈ›irea CV-ului Ã®n vederea acestei poziÈ›ii
        4. Procentajul general de potrivire (evaluat pe o scarÄƒ de la 0 la 100%)
        5. FiÈ›i cÃ¢t mai concret, citaÈ›i cerinÈ›ele specifice din descrierea postului È™i punctele din CV.
        """

        try:
            with st.spinner("GenerÄƒm o analizÄƒ detaliatÄƒâ€¦"):
                data = {
                    "model": "deepseek-chat",
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.3
                }

                response = requests.post(url, headers=headers, json=data)
                response.raise_for_status()

                result = response.json()
                analysis = result['choices'][0]['message']['content']

                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ² ÑĞµÑÑĞ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ğ°Ğ´Ğ°Ğ»
                st.session_state.analysis = analysis

        except Exception as e:
            st.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ Ğº API: {str(e)}")

# ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, ĞµÑĞ»Ğ¸ Ğ¾Ğ½ ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ Ğ² ÑĞµÑÑĞ¸Ğ¸
if 'analysis' in st.session_state and st.session_state.analysis:
    st.markdown("## ğŸ“Š AnalizÄƒ detaliatÄƒ a conformitÄƒÈ›ii")
    st.markdown(st.session_state.analysis)

    # ĞšĞ½Ğ¾Ğ¿ĞºĞ° Ğ´Ğ»Ñ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ² docx
    def create_word_document(text):
        doc = Document()
        doc.add_heading('AnalizÄƒ detaliatÄƒ a conformitÄƒÈ›ii', 0)
        for line in text.split('\n'):
            if line.strip():
                if line.startswith('##'):
                    doc.add_heading(line.replace('##', '').strip(), level=1)
                elif line.startswith('#'):
                    doc.add_heading(line.replace('#', '').strip(), level=2)
                else:
                    doc.add_paragraph(line)
        return doc

    doc = create_word_document(st.session_state.analysis)
    bio = io.BytesIO()
    doc.save(bio)
    bio.seek(0)

    st.download_button(
        label="ğŸ’¾ DescarcÄƒ analiza (DOCX)",
        data=bio,
        file_name="analiza_potrivire.docx",
        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    )
else:
    st.info("ApÄƒsaÈ›i butonul pentru a genera analiza de potrivire.")

# --- Ğ˜Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ ---

def generate_interview_questions(cv_text):
    prompt = f"""
    GenereazÄƒ 10 Ã®ntrebÄƒri pentru un interviu introductiv pe baza acestui CV:
    {cv_text}

    CerinÈ›e:

    1. 3 Ã®ntrebÄƒri despre experienÈ›a profesionalÄƒ
    2. 2 Ã®ntrebÄƒri despre abilitÄƒÈ›ile tehnice
    3. 1 Ã®ntrebare despre punctele slabe
    4. 1 Ã®ntrebare despre motivaÈ›ie
    5. 1 Ã®ntrebare despre aÈ™teptÄƒrile salariale
    6. 2 Ã®ntrebÄƒri biografice

    ÃntrebÄƒrile trebuie sÄƒ fie specifice È™i legate de CV

    ReturneazÄƒ doar o listÄƒ numerotatÄƒ de Ã®ntrebÄƒri, fÄƒrÄƒ explicaÈ›ii suplimentare.
    """

    response = requests.post(
        url,
        headers=headers,
        json={
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3
        }
    )
    return response.json()['choices'][0]['message']['content']

def generate_candidate_profile(questions, answers):
    prompt = f"""
    Pe baza acestor Ã®ntrebÄƒri È™i rÄƒspunsuri, creeazÄƒ un profil al candidatului:

    ÃntrebÄƒri:
    {questions}

    RÄƒspunsuri:

    {answers}

    Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ñ:
    ### ğŸ§‘â€ğŸ’» Portret profesional
    - CompetenÈ›e principale
    - ExperienÈ›Äƒ relevantÄƒ
    - ExpertizÄƒ tehnicÄƒ

    ### ğŸ¯ MotivaÈ›ie È™i obiective
    - Interese profesionale
    - AÈ™teptÄƒri de la job

    ### ğŸ“ˆ Puncte forte
    - Avantaje cheie
    - CompetenÈ›e unice

    ### âš ï¸ Zone de dezvoltare
    - Puncte slabe
    - CompetenÈ›e de Ã®mbunÄƒtÄƒÈ›it

    ### ğŸ’° AÈ™teptÄƒri privind compensaÈ›ia
    - AÈ™teptÄƒri salariale
    - Disponibilitate pentru negociere
    """

    response = requests.post(
        url,
        headers=headers,
        json={
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.2
        }
    )
    return response.json()['choices'][0]['message']['content']

st.title("ğŸ¤– AI HR-Recruiter: Interviu introductiv")

# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ
if 'interview_started' not in st.session_state:
    st.session_state.interview_started = False
    st.session_state.questions = None
    st.session_state.answers = {}
    st.session_state.profile = None

if not st.session_state.interview_started:
    if st.button("ğŸ¤ A trece interviul introductiv", type="primary"):
        with st.spinner("PregÄƒtim Ã®ntrebÄƒrile..."):
            st.session_state.questions = generate_interview_questions(documents[0])
            st.session_state.interview_started = True
        st.rerun()
        

if st.session_state.interview_started:
    st.success("Interviul a Ã®nceput! VÄƒ rog sÄƒ rÄƒspundeÈ›i la Ã®ntrebÄƒrile de mai jos.")

    questions_list = [q for q in st.session_state.questions.split('\n') if q.strip()]
    for i, question in enumerate(questions_list[:10]):
        st.session_state.answers[i] = st.text_area(
            label=f"**{i+1}:** {question}",
            value=st.session_state.answers.get(i, ""),
            key=f"answer_{i}"
        )

    if st.button("âœ… Interviul s-a Ã®ncheiat", type="primary"):
        with st.spinner("AnalizÄƒm rÄƒspunsurile..."):
            formatted_answers = "\n".join(
                [f"{i+1}. {q}\n   ĞÑ‚Ğ²ĞµÑ‚: {st.session_state.answers[i]}"
                 for i, q in enumerate(questions_list[:10])]
            )

            st.session_state.profile = generate_candidate_profile(
                st.session_state.questions,
                formatted_answers
            )

        st.success("Interviul s-a Ã®ncheiat!")
        st.balloons()
        # Ğ§Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ ÑÑ€Ğ°Ğ·Ñƒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»ÑÑ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾ĞºĞ¾Ğ½Ñ‡Ğ°Ğ½Ğ¸Ñ, Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼
        st.rerun()

    

    st.success("Interviul s-a Ã®ncheiat!")
    st.balloons()
    st.rerun()


      

if st.session_state.profile:
    st.markdown("## ğŸ“Œ Profilul candidatului")
    st.markdown(st.session_state.profile)

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ DOCX Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ñ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ°
    def create_word_document_profile(profile_text):
        doc = Document()
        doc.add_heading('Profil Candidat', 0)
        for line in profile_text.split('\n'):
            if line.strip():
                if line.startswith('###'):
                    doc.add_heading(line.replace('###', '').strip(), level=2)
                else:
                    doc.add_paragraph(line)
        return doc

    doc_profile = create_word_document_profile(st.session_state.profile)
    bio_profile = io.BytesIO()
    doc_profile.save(bio_profile)
    bio_profile.seek(0)

    st.download_button(
        label="ğŸ’¾ DescarcÄƒ profilul candidatului (DOCX)",
        data=bio_profile,
        file_name="profil_candidat.docx",
        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    )

