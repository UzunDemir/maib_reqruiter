import os
import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import json
from PyPDF2 import PdfReader
import tempfile
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import docx
from concurrent.futures import ThreadPoolExecutor

st.set_page_config(layout="wide", initial_sidebar_state="expanded")

# --- Stiluri »ôi bara lateralƒÉ ---
st.markdown("""
    <style>
        section[data-testid="stSidebar"] {
            background-color: #253646 !important;
        }
        .sidebar-title {
            color: white;
            font-size: 24px;
            font-weight: bold;
            text-align: center;
            margin-bottom: 1rem;
        }
        .sidebar-text {
            color: white;
        }
        #MainMenu, footer, header {
            display: none !important;
        }
        .center {
            display: flex;
            justify-content: center;
            align-items: center;
            text-align: center;
            flex-direction: column;
            margin-top: 0vh;
        }
        .match-card {
            border-radius: 10px;
            padding: 15px;
            margin-bottom: 15px;
            background-color: #f0f2f6;
        }
        .match-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        .progress-bar {
            height: 10px;
            background-color: #e0e0e0;
            border-radius: 5px;
            margin-top: 5px;
        }
        .progress-fill {
            height: 100%;
            border-radius: 5px;
            background-color: #40c1ac;
        }
    </style>
    <div class="center">
        <img src="https://www.maib.md/uploads/custom_blocks/image_1633004921_8nR1jw3Qfu_auto__0.png" width="300">
        <h1>HR-Recruiter MAIB</h1>
    </div>
""", unsafe_allow_html=True)

import streamlit as st

st.sidebar.markdown('<div class="sidebar-title">Proiect: AI Recruiter pentru MAIB</div>', unsafe_allow_html=True)
st.sidebar.divider()
st.sidebar.markdown("""
<div class="sidebar-text">

1. üì• **√éncƒÉrcarea posturilor vacante**  

   *Agentul √ÆncarcƒÉ automat toate posturile vacante actuale de la MAIB.*

2. üìÑ **CV-ul utilizatorului**  

   *Utilizatorul √Æ»ôi √ÆncarcƒÉ CV-ul pentru analizƒÉ.*

3. ü§ñ **CƒÉutarea posturilor potrivite**  

   *Agentul analizeazƒÉ CV-ul »ôi identificƒÉ **top 3 posturi** relevante pentru experien»õa »ôi competen»õele candidatului.*

4. üîç **Analiza celei mai relevante pozi»õii**  

   * *Eviden»õiazƒÉ **punctele forte** ale candidatului.*  
   * *IdentificƒÉ **punctele slabe** sau lipsurile √Æn competen»õe.*

5. ‚úÖ **Acordul candidatului**  

   *DacƒÉ este interesat, candidatul √Æ»ôi exprimƒÉ acordul pentru a continua procesul.*

6. üó£Ô∏è **Primul interviu (general)**  

   *Agentul pune √ÆntrebƒÉri generale, analizeazƒÉ rƒÉspunsurile »ôi formuleazƒÉ **primele concluzii**.*

7. üíª **Interviul tehnic**  

   *Evaluarea competen»õelor tehnice »ôi furnizarea unui **feedback tehnic**.*

8. üìã **Concluzia finalƒÉ**  

   *Agentul oferƒÉ un verdict final: **recomandare pentru angajare** sau **refuz argumentat**.*

</div>
""", unsafe_allow_html=True)

st.divider()


class DocumentChunk:
    def __init__(self, text, doc_name, page_num):
        self.text = text
        self.doc_name = doc_name
        self.page_num = page_num

class KnowledgeBase:
    def __init__(self):
        self.chunks = []
        self.uploaded_files = []
        self.doc_texts = []
    
    def clear(self):
        self.chunks = []
        self.doc_texts = []
        self.uploaded_files = []
    
    def split_text(self, text, max_chars=2000):
        chunks = []
        start = 0
        while start < len(text):
            end = min(start + max_chars, len(text))
            chunk = text[start:end].strip()
            if chunk:  # Skip empty chunks
                chunks.append(chunk)
            start = end
        return chunks

    def load_pdf(self, file_content, file_name):
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(file_content)
                tmp_file_path = tmp_file.name

            with open(tmp_file_path, 'rb') as file:
                reader = PdfReader(file)
                for page_num, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    if page_text:
                        for chunk in self.split_text(page_text):
                            self.chunks.append(DocumentChunk(chunk, file_name, page_num+1))
                            self.doc_texts.append(chunk)
            self.uploaded_files.append(file_name)
            return True
        except Exception as e:
            st.error(f"Eroare la √ÆncƒÉrcarea PDF: {str(e)}")
            return False
        finally:
            if os.path.exists(tmp_file_path):
                os.unlink(tmp_file_path)

    def load_docx(self, file_content, file_name):
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp_file:
                tmp_file.write(file_content)
                tmp_file_path = tmp_file.name

            doc = docx.Document(tmp_file_path)
            full_text = []
            for para in doc.paragraphs:
                text = para.text.strip()
                if text:  # Skip empty paragraphs
                    full_text.append(text)
            text = "\n".join(full_text)
            for chunk in self.split_text(text):
                self.chunks.append(DocumentChunk(chunk, file_name, 0))
                self.doc_texts.append(chunk)
            self.uploaded_files.append(file_name)
            return True
        except Exception as e:
            st.error(f"Eroare la √ÆncƒÉrcarea DOCX: {str(e)}")
            return False
        finally:
            if os.path.exists(tmp_file_path):
                os.unlink(tmp_file_path)

    def load_txt(self, file_content, file_name):
        try:
            text = file_content.decode('utf-8', errors='ignore')
            for chunk in self.split_text(text):
                self.chunks.append(DocumentChunk(chunk, file_name, 0))
                self.doc_texts.append(chunk)
            self.uploaded_files.append(file_name)
            return True
        except Exception as e:
            st.error(f"Eroare la √ÆncƒÉrcarea TXT: {str(e)}")
            return False

    def load_file(self, uploaded_file):
        name = uploaded_file.name.lower()
        content = uploaded_file.read()
        if name.endswith('.pdf'):
            return self.load_pdf(content, uploaded_file.name)
        elif name.endswith('.docx'):
            return self.load_docx(content, uploaded_file.name)
        elif name.endswith('.txt'):
            return self.load_txt(content, uploaded_file.name)
        else:
            st.warning(f"Formatul fi»ôierului {uploaded_file.name} nu este suportat.")
            return False

    def get_all_text(self):
        return "\n\n".join(self.doc_texts)

# Ini»õializare baza de cuno»ôtin»õe
if 'knowledge_base' not in st.session_state:
    st.session_state.knowledge_base = KnowledgeBase()

if 'vacancies_data' not in st.session_state:
    st.session_state.vacancies_data = []

##############################
# √éncƒÉrcare oferte de pe rabota.md
def scrape_vacancy(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
        soup_vac = BeautifulSoup(resp.text, 'html.parser')

        title_tag = soup_vac.find('h1')
        title = title_tag.get_text(strip=True) if title_tag else 'Titlu indisponibil'

        vacancy_content = soup_vac.find('div', class_='vacancy-content')
        description = vacancy_content.get_text(separator='\n', strip=True) if vacancy_content else 'Descriere indisponibilƒÉ'

        return {'url': url, 'title': title, 'description': description}
    except Exception as e:
        st.error(f"Eroare la preluarea ofertei {url}: {str(e)}")
        return None

def load_vacancies():
    base_url = "https://www.rabota.md/ru/companies/moldova-agroindbank#vacancies"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    with st.spinner("Se √ÆncarcƒÉ ofertele de muncƒÉ..."):
        try:
            response = requests.get(base_url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            links = soup.find_all('a', class_='vacancyShowPopup')
            urls = [urljoin(base_url, a['href']) for a in links]

            vacancies_data = []
            progress_bar = st.progress(0)
            status_text = st.empty()

            # Use threading to speed up scraping
            with ThreadPoolExecutor(max_workers=5) as executor:
                results = list(executor.map(scrape_vacancy, urls))
            
            for i, result in enumerate(results):
                if result:
                    vacancies_data.append(result)
                    progress = (i + 1) / len(urls)
                    progress_bar.progress(progress)
                    status_text.text(f"[{i+1}/{len(urls)}] OfertƒÉ √ÆncƒÉrcatƒÉ: {result['title']}")

            st.session_state.vacancies_data = [v for v in vacancies_data if v is not None]
            st.success(f"Au fost √ÆncƒÉrcate {len(st.session_state.vacancies_data)} oferte de muncƒÉ!")
            
        except Exception as e:
            st.error(f"Eroare la √ÆncƒÉrcarea ofertelor: {str(e)}")

if st.button("√éncarcƒÉ ofertele de muncƒÉ de pe rabota.md"):
    load_vacancies()

# Afi»ôare lista oferte √Æn bara lateralƒÉ
if st.session_state.vacancies_data:
    with st.sidebar:
        st.markdown("### üîé Lista ofertelor MAIB:")
        st.success(f"Oferte gƒÉsite: {len(st.session_state.vacancies_data)}")
        for vac in st.session_state.vacancies_data:
            st.markdown(
                f'<a href="{vac["url"]}" target="_blank" style="color:#40c1ac; text-decoration:none;">‚Ä¢ {vac["title"]}</a>',
                unsafe_allow_html=True
            )

##############################
# √éncƒÉrcare CV
st.markdown("### üìÑ √éncƒÉrcƒÉ CV-ul tƒÉu (PDF, DOCX sau TXT)")
uploaded_files = st.file_uploader("SelecteazƒÉ fi»ôier(e)", type=['pdf', 'docx', 'txt'], accept_multiple_files=True)

if uploaded_files:
    kb = st.session_state.knowledge_base
    kb.clear()  # CurƒÉ»õƒÉ con»õinutul anterior
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, uploaded_file in enumerate(uploaded_files):
        success = kb.load_file(uploaded_file)
        progress = (i + 1) / len(uploaded_files)
        progress_bar.progress(progress)
        status_text.text(f"Se proceseazƒÉ fi»ôierul {i+1}/{len(uploaded_files)}: {uploaded_file.name}")
    
    st.session_state.knowledge_base = kb
    st.success(f"Au fost √ÆncƒÉrcate {len(uploaded_files)} fi»ôiere!")

if not st.session_state.knowledge_base.uploaded_files:
    st.info("Te rugƒÉm sƒÉ √Æncarci un CV pentru analizƒÉ")
    st.stop()

##############################
# Analiza potrivirilor
st.markdown("### üîç Cele mai relevante oferte pentru CV-ul tƒÉu")

cv_text = st.session_state.knowledge_base.get_all_text()
vacancies = st.session_state.vacancies_data

if not vacancies:
    st.warning("Nu existƒÉ oferte de muncƒÉ disponibile. Te rugƒÉm sƒÉ √Æncarci ofertele mai √Ænt√¢i.")
    st.stop()

# Procesare avansatƒÉ a textelor
vacancy_texts = [f"{vac['title']}\n{vac['description']}" for vac in vacancies]
documents = [cv_text] + vacancy_texts

# TF-IDF √ÆmbunƒÉtƒÉ»õit
vectorizer = TfidfVectorizer(
    stop_words=None,
    ngram_range=(1, 2),  # Include bigrame pentru mai mult context
    max_features=5000
)

# try:
#     with st.spinner("Se analizeazƒÉ potrivirile..."):
#         tfidf_matrix = vectorizer.fit_transform(documents)
#         similarity_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()
    
#     # NormalizeazƒÉ scorurile √Æntre 0 »ôi 100 pentru o afi»ôare mai bunƒÉ
#     normalized_scores = (similarity_scores - similarity_scores.min()) / (similarity_scores.max() - similarity_scores.min()) * 100
#     normalized_scores = np.clip(normalized_scores, 0, 100)
    
#     # Ob»õine top 3 oferte
#     top_indices = similarity_scores.argsort()[::-1][:3]
    
#     for idx in top_indices:
#         vac = vacancies[idx]
#         score = normalized_scores[idx]
        
#         with st.container():
#             st.markdown(f"""
#             <div class="match-card">
#                 <div class="match-header">
#                     <h3>{vac['title']}</h3>
#                     <h4>{score:.0f}% potrivire</h4>
#                 </div>
#                 <div class="progress-bar">
#                     <div class="progress-fill" style="width: {score}%"></div>
#                 </div>
#                 <p><a href="{vac['url']}" target="_blank">üîó Vezi oferta completƒÉ</a></p>
#             </div>
#             """, unsafe_allow_html=True)
            
#             with st.expander("üìù Detalii despre ofertƒÉ"):
#                 st.write(vac['description'])
                
#             st.write("---")

# except Exception as e:
#     st.error(f"Eroare la analiza potrivirilor: {str(e)}")

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import streamlit as st

try:
    with st.spinner("Se analizeazƒÉ potrivirile..."):
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(documents)
        similarity_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()

    # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
    score_min, score_max = similarity_scores.min(), similarity_scores.max()
    if score_max - score_min > 0:
        normalized_scores = (similarity_scores - score_min) / (score_max - score_min) * 100
    else:
        normalized_scores = np.zeros_like(similarity_scores)

    normalized_scores = np.clip(normalized_scores, 0, 100)

    top_indices = similarity_scores.argsort()[::-1][:3]

    for idx in top_indices:
        vac = vacancies[idx]
        score = normalized_scores[idx]

        with st.container():
            st.markdown(f"""
            <div class="match-card">
                <div class="match-header">
                    <h3>{vac['title']}</h3>
                    <h4>{score:.0f}% potrivire</h4>
                </div>
                <div class="progress-bar">
                    <div class="progress-fill" style="width: {score}%"></div>
                </div>
                <p><a href="{vac['url']}" target="_blank">üîó Vezi oferta completƒÉ</a></p>
            </div>
            """, unsafe_allow_html=True)

            with st.expander("üìù Detalii despre ofertƒÉ"):
                st.write(vac['description'])

            st.write("---")

except Exception as e:
    st.error(f"Eroare la analiza potrivirilor: {str(e)}")

    ###########################################################

    # –ü–æ–ª—É—á–µ–Ω–∏–µ API –∫–ª—é—á–∞
api_key = st.secrets.get("DEEPSEEK_API_KEY")
if not api_key:
    st.error("API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–±–∞–≤—å—Ç–µ –µ–≥–æ –≤ Secrets.")
    st.stop()

url = "https://api.deepseek.com/v1/chat/completions"
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json"
}

# –í—ã–±–∏—Ä–∞–µ–º —Å–∞–º—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –≤–∞–∫–∞–Ω—Å–∏—é
if len(top_indices) > 0:
    best_match_idx = top_indices[0]
    best_match_vacancy = vacancies[best_match_idx]

    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞
    if st.button("üîç GenereazƒÉ analiza de potrivire"):
    
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            prompt = f"""
            AnalizeazƒÉ coresponden»õa dintre CV-ul candidatului »ôi oferta de muncƒÉ.
            Mai √Ænt√¢i voi furniza CV-ul, apoi descrierea postului.
        
            CV-ul candidatului:
            {cv_text}
            
            Descrierea postului:
            {best_match_vacancy['description']}
            
            VƒÉ rog sƒÉ efectua»õi analiza conform urmƒÉtoarei structuri:
        
            1. Punctele forte ale CV-ului (potrivirea exactƒÉ cu cerin»õele postului)
            2. Punctele slabe sau lacunele din CV (unde candidatul nu corespunde)
            3. RecomandƒÉri concrete pentru √ÆmbunƒÉtƒÉ»õirea CV-ului √Æn vederea acestei pozi»õii
            4. Procentajul general de potrivire (evaluat pe o scarƒÉ de la 0 la 100%)
            5. Fi»õi c√¢t mai concret, cita»õi cerin»õele specifice din descrierea postului »ôi punctele din CV.
            """
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ API
            try:
                with st.spinner("GenerƒÉm o analizƒÉ detaliatƒÉ‚Ä¶"):
                    data = {
                        "model": "deepseek-chat",
                        "messages": [{"role": "user", "content": prompt}],
                        "temperature": 0.3
                    }
                    
                    response = requests.post(url, headers=headers, json=data)
                    response.raise_for_status()
                    
                    result = response.json()
                    analysis = result['choices'][0]['message']['content']
                    
                    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    st.markdown("## üìä AnalizƒÉ detaliatƒÉ a conformitƒÉ»õii")
                    st.markdown(analysis)
                    
                    
                    
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {str(e)}")
        else:
            st.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

#######################################################

import streamlit as st
import requests
import json
from time import sleep

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
def generate_interview_questions(cv_text):
    prompt = f"""
    –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 10 –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–∑–Ω–∞–∫–æ–º–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ —Ä–µ–∑—é–º–µ:
    {cv_text}
    
    –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    1. 3 –≤–æ–ø—Ä–æ—Å–æ–≤ –æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–º –æ–ø—ã—Ç–µ
    2. 2 –≤–æ–ø—Ä–æ—Å–∞ –æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –Ω–∞–≤—ã–∫–∞—Ö
    3. 1 –≤–æ–ø—Ä–æ—Å –æ —Å–ª–∞–±—ã—Ö —Å—Ç–æ—Ä–æ–Ω–∞—Ö
    4. 1 –≤–æ–ø—Ä–æ—Å –æ –º–æ—Ç–∏–≤–∞—Ü–∏–∏
    5. 1 –≤–æ–ø—Ä–æ—Å –æ –∑–∞—Ä–ø–ª–∞—Ç–Ω—ã—Ö –æ–∂–∏–¥–∞–Ω–∏—è—Ö
    6. 2 –±–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–∞
    6. –í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ —Å —Ä–µ–∑—é–º–µ
    
    –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—è—Å–Ω–µ–Ω–∏–π.
    """
    
    response = requests.post(
        url,
        headers=headers,
        json={
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3
        }
    )
    return response.json()['choices'][0]['message']['content']

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è
def generate_candidate_profile(questions, answers):
    prompt = f"""
    –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å–æ—Å—Ç–∞–≤—å –ø—Ä–æ—Ñ–∏–ª—å –∫–∞–Ω–¥–∏–¥–∞—Ç–∞:
    
    –í–æ–ø—Ä–æ—Å—ã:
    {questions}
    
    –û—Ç–≤–µ—Ç—ã:
    {answers}
    
    –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ—Ñ–∏–ª—è:
    ### üßë‚Äçüíª –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–æ—Ä—Ç—Ä–µ—Ç
    - –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–≤—ã–∫–∏
    - –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –æ–ø—ã—Ç
    - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞
    
    ### üéØ –ú–æ—Ç–∏–≤–∞—Ü–∏—è –∏ —Ü–µ–ª–∏
    - –ö–∞—Ä—å–µ—Ä–Ω—ã–µ –∏–Ω—Ç–µ—Ä–µ—Å—ã
    - –û–∂–∏–¥–∞–Ω–∏—è –æ—Ç —Ä–∞–±–æ—Ç—ã
    
    ### üìà –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã
    - –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞
    - –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
    
    ### ‚ö†Ô∏è –ó–æ–Ω—ã —Ä–∞–∑–≤–∏—Ç–∏—è
    - –°–ª–∞–±—ã–µ –º–µ—Å—Ç–∞
    - –ù–∞–≤—ã–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è
    
    ### üí∞ –ö–æ–º–ø–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω—ã–µ –æ–∂–∏–¥–∞–Ω–∏—è
    - –ó–∞—Ä–ø–ª–∞—Ç–Ω—ã–µ –æ–∂–∏–¥–∞–Ω–∏—è
    - –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ negotiation
    """
    
    response = requests.post(
        url,
        headers=headers,
        json={
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.2
        }
    )
    return response.json()['choices'][0]['message']['content']

# –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.title("ü§ñ HR-–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: –û–∑–Ω–∞–∫–æ–º–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ")

if 'interview_started' not in st.session_state:
    st.session_state.interview_started = False
    st.session_state.questions = None
    st.session_state.answers = {}
    st.session_state.profile = None

# –ó–∞–ø—É—Å–∫ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ –∫–Ω–æ–ø–∫–µ
if not st.session_state.interview_started:
    if st.button("üé§ –ü—Ä–æ–π—Ç–∏ –æ–∑–Ω–∞–∫–æ–º–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ", type="primary"):
        with st.spinner("–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã..."):
            st.session_state.questions = generate_interview_questions(documents[0])
            st.session_state.interview_started = True
        st.rerun()

# –ï—Å–ª–∏ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞—Ç–æ
if st.session_state.interview_started:
    st.success("–°–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞—Ç–æ! –û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∏–∂–µ.")
    
    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã –∏ –ø–æ–ª—è –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤
    questions_list = [q for q in st.session_state.questions.split('\n') if q.strip()]
    for i, question in enumerate(questions_list[:10]):
        st.session_state.answers[i] = st.text_area(
            label=f"**–í–æ–ø—Ä–æ—Å {i+1}:** {question}",
            value=st.session_state.answers.get(i, ""),
            key=f"answer_{i}"
        )
    
    # –ö–Ω–æ–ø–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
    if st.button("‚úÖ –ó–∞–≤–µ—Ä—à–∏—Ç—å —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ", type="primary"):
        with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã..."):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç—ã –≤ —É–¥–æ–±–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            formatted_answers = "\n".join(
                [f"{i+1}. {q}\n   –û—Ç–≤–µ—Ç: {st.session_state.answers[i]}" 
                 for i, q in enumerate(questions_list[:10])]
            )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ—Ñ–∏–ª—å
            st.session_state.profile = generate_candidate_profile(
                st.session_state.questions,
                formatted_answers
            )
            
        st.success("–°–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        st.balloons()
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ—Ñ–∏–ª—å
        st.markdown("## üìå –ü—Ä–æ—Ñ–∏–ª—å –∫–∞–Ω–¥–∏–¥–∞—Ç–∞")
        st.markdown(st.session_state.profile)
        
        # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        st.download_button(
            label="üíæ –°–∫–∞—á–∞—Ç—å –ø—Ä–æ—Ñ–∏–ª—å",
            data=st.session_state.profile,
            file_name="candidate_profile.md",
            mime="text/markdown"
        )
        
        # –ö–Ω–æ–ø–∫–∞ –Ω–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ
        if st.button("üîÑ –ü—Ä–æ–π—Ç–∏ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –µ—â–µ —Ä–∞–∑"):
            st.session_state.interview_started = False
            st.session_state.questions = None
            st.session_state.answers = {}
            st.session_state.profile = None
            st.rerun()
