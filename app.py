import os
import streamlit as st
import requests
import json
import time
from PyPDF2 import PdfReader
import tempfile
from datetime import datetime
from transformers import AutoTokenizer
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
#tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-llm")
from transformers import GPT2Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Streamlit
st.set_page_config(layout="wide", initial_sidebar_state="auto")
st.markdown("""
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<style>
    .css-1jc7ptx, .e1ewe7hr3, .viewerBadge_container__1QSob, 
    .styles_viewerBadge__1yB5_, .viewerBadge_link__1S137, 
    .viewerBadge_text__1JaDK, #MainMenu, footer, header { 
        display: none !important; 
    }
    .center {
        display: flex;
        justify-content: center;
        align-items: center;
        text-align: center;
        flex-direction: column;
        margin-top: 0vh;
    }
</style>
""", unsafe_allow_html=True)
#################################

# st.sidebar.write("[Uzun Demir](https://uzundemir.github.io/)") #[Github](https://github.com/UzunDemir)     [Linkedin](https://www.linkedin.com/in/uzundemir/)     
# st.sidebar.write("[Github](https://github.com/UzunDemir)")
# st.sidebar.write("[Linkedin](https://www.linkedin.com/in/uzundemir/)")
st.sidebar.title("–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞")
st.sidebar.title("TEST-passer (AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ —Ç–µ—Å—Ç–∞–º)")
st.sidebar.divider()
st.sidebar.write(
        """
                                       
                     –≠—Ç–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –≤ —Ü–µ–ª—è—Ö –ø–æ–º–æ—â–∏ —Å—Ç—É–¥–µ–Ω—Ç–∞–º –ø—Ä–∏ —Å–¥–∞—á–µ —Ç–µ—Å—Ç–æ–≤ –ø–æ –õ–Æ–ë–û–ô –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π —Ç–µ–º–µ.                  

     1. –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç? 
     
        –°—Ç—É–¥–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–∞–µ—Ç —É—á–µ–±–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª –≤ pdf. TEST-passer –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —Ç–µ—Å—Ç—ã, –≤—ã–±–∏—Ä–∞—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã. –¢–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å–æ—Å—Ç–∞–≤–∏–ª–∞ 88%.
     
     2. –ü–æ—á–µ–º—É –Ω–µ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –æ–±—ã—á–Ω—ã–º–∏ —á–∞—Ç–∞–º–∏ (GPT, DeepSeek –∏ —Ç. –¥.)? 
     
        –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ —á—Ç–æ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –æ–≥—Ä–æ–º–Ω–æ–º 
        –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –æ–Ω–∞ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫, –∞ –ª–∏—à—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç "–≤–µ—Ä–æ—è—Ç–Ω—ã–π —Å–ª–µ–¥—É—é—â–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞". 
        –û–Ω–∞ —Ç–∞–∫–∂–µ –∏–º–µ–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å "–≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä–æ–≤–∞—Ç—å", —Ç–æ –µ—Å—Ç—å –º–æ–∂–µ—Ç "–ø—Ä–∏–¥—É–º–∞—Ç—å" —Ñ–∞–∫—Ç, –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–ª–∏ —Ç–µ—Ä–º–∏–Ω, –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ –∑–≤—É—á–∏—Ç –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ.
        –ü–æ—ç—Ç–æ–º—É –Ω–∞–∏–±–æ–ª–µ–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –±—É–¥–µ—Ç –≤—ã–¥–∞–≤–∞—Ç—å –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –ù–£–ñ–ù–´–ô –∏–∞—Ç–µ—Ä–∏–∞–ª. 
        
     3. –ß—Ç–æ –¥–µ–ª–∞–µ—Ç –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ?    
     
        * –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç pdf-—Ñ–∞–π–ª—ã (–ª—é–±—ã–µ –∫—É—Ä—Å—ã, –ø—Ä–µ–¥–º–µ—Ç—ã, —Ç–µ–º—ã)
        * –°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        * –ü—Ä–∏–º–µ–Ω—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–∏–Ω–≥ (–¥–µ–ª–∏—Ç –ø–æ —Å–º—ã—Å–ª–æ–≤—ã–º –≥—Ä–∞–Ω–∏—Ü–∞–º)
        * –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (HyDE + –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞) 
          (–∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –¥–≤–∞ –º–µ—Ç–æ–¥–∞ –ø–æ–∏—Å–∫–∞, —á—Ç–æ–±—ã –Ω–∞—Ö–æ–¥–∏—Ç—å –æ—Ç–≤–µ—Ç—ã, –µ—Å–ª–∏ –æ–Ω —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω –∏–Ω–∞—á–µ —á–µ–º –≤ —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö)
        * –í–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤
        * –ù–∞—Å—Ç—Ä–æ–π–∫–∞ DeepSeek –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏) 

                        

    4. –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–æ—Ä–∞–±–æ—Ç–∞–Ω–æ. –ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:

    –ß–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:

        * –¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ –±–ª–æ–∫–∏ (–ø–æ –∞–±–∑–∞—Ü–∞–º) —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ–∫–µ–Ω–æ–≤

        * –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä DeepSeek –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤

    –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤:

        * –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω TF-IDF + –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞—Å—Ç–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞

        * –í –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ø–∞–¥–∞—é—Ç —Ç–æ–ª—å–∫–æ 3 –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞

    –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç:

        * –ß–µ—Ç–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –æ—Ç–≤–µ—á–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º

        * –î–æ–±–∞–≤–ª–µ–Ω—ã —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–¥–æ–∫—É–º–µ–Ω—Ç –∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞)

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã API:

        * –£–º–µ–Ω—å—à–µ–Ω–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (temperature=0.1) –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤

        * –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ

    –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:

        * –î–æ–∫—É–º–µ–Ω—Ç—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ

        * –í API –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞—Å—Ç–∏
                     
    5. –ë—É–¥—É—Ç –ª–∏ –¥–æ—Ä–∞–±–æ—Ç–∫–∏?
    
    –î–∞, –±—É–¥—É—Ç:
    
    * –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ –≤–∏–¥–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
    * –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π (–∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ) –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
    * —É–º–µ–Ω—å—à–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ–∏—Å–∫–∞ –æ—Ç–≤–µ—Ç–∞ 
                     
                        
                     """
    )

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç–∏–ª—å –¥–ª—è —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤
st.markdown("""
    <style>
    .center {
        display: flex;
        justify-content: center;
        align-items: center;
        /height: 5vh;
        text-align: center;
        flex-direction: column;
        margin-top: 0vh;  /* –æ—Ç—Å—Ç—É–ø —Å–≤–µ—Ä—Ö—É */
    }
    .github-icon:hover {
        color: #4078c0; /* –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–≤–µ—Ç–∞ –ø—Ä–∏ –Ω–∞–≤–µ–¥–µ–Ω–∏–∏ */
    }
    </style>
    <div class="center">
        <img src="https://github.com/UzunDemir/mnist_777/blob/main/200w.gif?raw=true">
        <h1>TEST-passer</h1>
        <h2>AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ —Ç–µ—Å—Ç–∞–º</h2>
        <p> (—Å—Ç—Ä–æ–≥–æ –ø–æ —É—á–µ–±–Ω—ã–º –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º)</p>
    </div>
    """, unsafe_allow_html=True)

st.divider()
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∫–∞–Ω–≤—ã
stroke_width = 10
stroke_color = "black"
bg_color = "white"
drawing_mode = "freedraw"

# –ü–æ–ª—É—á–µ–Ω–∏–µ API –∫–ª—é—á–∞
api_key = st.secrets.get("DEEPSEEK_API_KEY")
if not api_key:
    st.error("API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–±–∞–≤—å—Ç–µ –µ–≥–æ –≤ Secrets.")
    st.stop()

url = "https://api.deepseek.com/v1/chat/completions"
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json"
}

class DocumentChunk:
    def __init__(self, text, doc_name, page_num):
        self.text = text
        self.doc_name = doc_name
        self.page_num = page_num
        self.embedding = None

class KnowledgeBase:
    def __init__(self):
        self.chunks = []
        self.uploaded_files = []
        self.vectorizer = TfidfVectorizer(stop_words='english')
        self.tfidf_matrix = None
        self.doc_texts = []
    
    def split_text(self, text, max_tokens=2000):
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            tokens = tokenizer.tokenize(para)
            if len(tokenizer.tokenize(current_chunk + para)) > max_tokens:
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = para
                else:
                    chunks.append(para)
                    current_chunk = ""
            else:
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para
        
        if current_chunk:
            chunks.append(current_chunk)
            
        return chunks
    
    def load_pdf(self, file_content, file_name):
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(file_content)
                tmp_file_path = tmp_file.name
            
            with open(tmp_file_path, 'rb') as file:
                reader = PdfReader(file)
                for page_num, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    if page_text:
                        chunks = self.split_text(page_text)
                        for chunk in chunks:
                            self.chunks.append(DocumentChunk(
                                text=chunk,
                                doc_name=file_name,
                                page_num=page_num + 1
                            ))
                            self.doc_texts.append(chunk)
                
                if self.chunks:
                    self.uploaded_files.append(file_name)
                    # –û–±–Ω–æ–≤–ª—è–µ–º TF-IDF –º–∞—Ç—Ä–∏—Ü—É
                    self.tfidf_matrix = self.vectorizer.fit_transform(self.doc_texts)
                    return True
                else:
                    st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ {file_name}")
                    return False
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ PDF: {e}")
            return False
        finally:
            if os.path.exists(tmp_file_path):
                os.unlink(tmp_file_path)
    
    def find_most_relevant_chunks(self, query, top_k=3):
        """–ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ —Å –ø–æ–º–æ—â—å—é TF-IDF –∏ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        if not self.chunks:
            return []
            
        query_vec = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vec, self.tfidf_matrix)
        top_indices = np.argsort(similarities[0])[-top_k:][::-1]
        
        return [(self.chunks[i].text, self.chunks[i].doc_name, self.chunks[i].page_num) 
                for i in top_indices if similarities[0][i] > 0.1]
    
    def get_document_names(self):
        return self.uploaded_files

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
if 'knowledge_base' not in st.session_state:
    st.session_state.knowledge_base = KnowledgeBase()

if 'messages' not in st.session_state:
    st.session_state.messages = []

# # –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å
# st.markdown("""
# <div class="center">
#     <h1>TEST-passer</h1>
#     <h2>AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ —Ç–µ—Å—Ç–∞–º</h2>
#     <p>(—Å—Ç—Ä–æ–≥–æ –ø–æ —É—á–µ–±–Ω—ã–º –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º)</p>
# </div>
# """, unsafe_allow_html=True)



# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
uploaded_files = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å —É—á–µ–±–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –≤ PDF", type="pdf", accept_multiple_files=True)
if uploaded_files:
    for uploaded_file in uploaded_files:
        if uploaded_file.name not in st.session_state.knowledge_base.get_document_names():
            success = st.session_state.knowledge_base.load_pdf(uploaded_file.getvalue(), uploaded_file.name)
            if success:
                st.success(f"–§–∞–π–ª {uploaded_file.name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
if st.session_state.knowledge_base.get_document_names():
    st.subheader("üìö –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:")
    for doc in st.session_state.knowledge_base.get_document_names():
        st.markdown(f"- {doc}")
else:
    st.info("‚ÑπÔ∏è –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# –í–≤–æ–¥ –≤–æ–ø—Ä–æ—Å–∞
if prompt := st.chat_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # –ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    relevant_chunks = st.session_state.knowledge_base.find_most_relevant_chunks(prompt)
    
    if not relevant_chunks:
        response_text = "–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö ‚ùå"
        st.session_state.messages.append({"role": "assistant", "content": response_text})
        with st.chat_message("assistant"):
            st.markdown(response_text)
    else:
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        context = "\n\n".join([f"–î–æ–∫—É–º–µ–Ω—Ç: {doc_name}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}\n{text}" 
                             for text, doc_name, page_num in relevant_chunks])
        
        full_prompt = f"""Answer strictly based on the educational materials provided below.
     Respond in the same language the question is written in.
     If the answer is not found in the materials, reply with: 'Answer not found in the materials'.
    
    
        
        educational materials: {prompt}
        
        relevant materials:
        {context}"""
        
        data = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": full_prompt}],
            "max_tokens": 2000,
            "temperature": 0.1  # –£–º–µ–Ω—å—à–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤
        }
        
        with st.spinner("–ò—â–µ–º –æ—Ç–≤–µ—Ç..."):
            start_time = datetime.now()
            
            try:
                response = requests.post(url, headers=headers, json=data)
                
                if response.status_code == 200:
                    response_data = response.json()
                    full_response = response_data['choices'][0]['message']['content']
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                    sources = "\n\n–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n" + "\n".join(
                        [f"- {doc_name}, —Å—Ç—Ä. {page_num}" for _, doc_name, page_num in relevant_chunks]
                    )
                    full_response += sources
                    
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                    with st.chat_message("assistant"):
                        st.markdown(full_response + " ‚úÖ")
                    
                    end_time = datetime.now()
                    duration = (end_time - start_time).total_seconds()
                    st.info(f"‚è±Ô∏è –ü–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞ –∑–∞–Ω—è–ª {duration:.2f} —Å–µ–∫—É–Ω–¥")
                else:
                    st.error(f"–û—à–∏–±–∫–∞ API: {response.status_code} - {response.text}")
            except Exception as e:
                st.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}")

# –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —á–∞—Ç–∞
if st.button("–û—á–∏—Å—Ç–∏—Ç—å —á–∞—Ç"):
    st.session_state.messages = []
    st.rerun()
