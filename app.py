import os
import streamlit as st
import requests
import json
import time
from PyPDF2 import PdfReader
import tempfile
from datetime import datetime
from transformers import AutoTokenizer
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import json


st.set_page_config(layout="wide", initial_sidebar_state="expanded")
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
#tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-llm")
from transformers import GPT2Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")



# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–æ—Ç–∏–ø–∞
# st.sidebar.image("https://www.maib.md/uploads/custom_blocks/image_1633004921_8nR1jw3Qfu_auto__0.png", use_container_width=True)

# –ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ HTML+JS –¥–ª—è –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏ + —Å–∫—Ä—ã—Ç–∏–µ –≤–µ—Ä—Ö–Ω–µ–≥–æ –º–µ–Ω—é, —Ñ—É—Ç–µ—Ä–∞ –∏ —Ö–µ–¥–µ—Ä–∞
st.markdown("""
    <style>
        /* –°–∞–π–¥–±–∞—Ä —Ü–µ–ª–∏–∫–æ–º */
        section[data-testid="stSidebar"] {
            background-color: #253646 !important;
        }

        /* –ó–∞–≥–æ–ª–æ–≤–∫–∏ –≤–Ω—É—Ç—Ä–∏ —Å–∞–π–¥–±–∞—Ä–∞ */
        .sidebar-title {
            color: white;
            font-size: 24px;
            font-weight: bold;
            text-align: center;
            margin-bottom: 1rem;
        }

        /* –¢–µ–∫—Å—Ç –≤ —Å–∞–π–¥–±–∞—Ä–µ */
        .sidebar-text {
            color: white;
        }

        /* –°–∫—Ä—ã–≤–∞–µ–º –≤–µ—Ä—Ö–Ω–µ–µ –º–µ–Ω—é, —Ñ—É—Ç–µ—Ä –∏ —Ö–µ–¥–µ—Ä Streamlit */
        #MainMenu, footer, header {
            display: none !important;
        }
    </style>
""", unsafe_allow_html=True)


# –¢–µ–∫—Å—Ç –≤ —Å–∞–π–¥–±–∞—Ä–µ
st.sidebar.markdown('<div class="sidebar-title">Proiect: AI Recruiter pentru MAIB</div>', unsafe_allow_html=True)
# st.sidebar.markdown('<div class="sidebar-title">HR-RECRUITER</div>', unsafe_allow_html=True)

st.sidebar.divider()

st.sidebar.markdown("""
<div class="sidebar-text">

1. üì• **√éncƒÉrcarea posturilor vacante**

   *Agentul √ÆncarcƒÉ automat toate posturile vacante actuale de la MAIB.*

2. üìÑ **CV-ul utilizatorului**

   *Utilizatorul √Æ»ôi √ÆncarcƒÉ CV-ul pentru analizƒÉ.*

3. ü§ñ **CƒÉutarea posturilor potrivite**

   * *Agentul analizeazƒÉ CV-ul »ôi identificƒÉ **top 3 posturi** relevante pentru experien»õa »ôi competen»õele candidatului.*

4. üîç **Analiza celei mai relevante pozi»õii**

   * *Eviden»õiazƒÉ **punctele forte** ale candidatului.*
   * *IdentificƒÉ **punctele slabe** sau lipsurile √Æn competen»õe.*

5. ‚úÖ **Acordul candidatului**

   *DacƒÉ este interesat, candidatul √Æ»ôi exprimƒÉ acordul pentru a continua procesul.*

6. üó£Ô∏è **Primul interviu (general)**

   *Agentul pune √ÆntrebƒÉri generale, analizeazƒÉ rƒÉspunsurile »ôi formuleazƒÉ **primele concluzii**.*

7. üíª **Interviul tehnic**

   *Evaluarea competen»õelor tehnice ale candidatului √Æn raport cu cerin»õele postului »ôi furnizarea unui **feedback tehnic**.*

8. üìã **Concluzia finalƒÉ**

   *Agentul oferƒÉ un verdict final: **recomandare pentru angajare** sau **refuz argumentat**.*

---
</div>
""", unsafe_allow_html=True)


# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç–∏–ª—å –¥–ª—è —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤
st.markdown("""
    <style>
    .center {
        display: flex;
        justify-content: center;
        align-items: center;
        /height: 5vh;
        text-align: center;
        flex-direction: column;
        margin-top: 0vh;  /* –æ—Ç—Å—Ç—É–ø —Å–≤–µ—Ä—Ö—É */
    }
    .github-icon:hover {
        color: #4078c0; /* –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–≤–µ—Ç–∞ –ø—Ä–∏ –Ω–∞–≤–µ–¥–µ–Ω–∏–∏ */
    }
    </style>
    <div class="center">
        <img src="https://www.maib.md/uploads/custom_blocks/image_1633004921_8nR1jw3Qfu_auto__0.png" width="300">
        <h1>HR-reqruiter</h1>        
    </div>
    """, unsafe_allow_html=True)

st.divider()
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∫–∞–Ω–≤—ã
stroke_width = 10
stroke_color = "black"
bg_color = "white"
drawing_mode = "freedraw"

# –ü–æ–ª—É—á–µ–Ω–∏–µ API –∫–ª—é—á–∞
api_key = st.secrets.get("DEEPSEEK_API_KEY")
if not api_key:
    st.error("API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–±–∞–≤—å—Ç–µ –µ–≥–æ –≤ Secrets.")
    st.stop()

url = "https://api.deepseek.com/v1/chat/completions"
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json"
}

class DocumentChunk:
    def __init__(self, text, doc_name, page_num):
        self.text = text
        self.doc_name = doc_name
        self.page_num = page_num
        self.embedding = None

# class KnowledgeBase:
#     def __init__(self):
#         self.chunks = []
#         self.uploaded_files = []
#         self.vectorizer = TfidfVectorizer(stop_words='english')
#         self.tfidf_matrix = None
#         self.doc_texts = []
    
#     def split_text(self, text, max_tokens=2000):
#         paragraphs = text.split('\n\n')
#         chunks = []
#         current_chunk = ""
        
#         for para in paragraphs:
#             para = para.strip()
#             if not para:
#                 continue
                
#             tokens = tokenizer.tokenize(para)
#             if len(tokenizer.tokenize(current_chunk + para)) > max_tokens:
#                 if current_chunk:
#                     chunks.append(current_chunk)
#                     current_chunk = para
#                 else:
#                     chunks.append(para)
#                     current_chunk = ""
#             else:
#                 if current_chunk:
#                     current_chunk += "\n\n" + para
#                 else:
#                     current_chunk = para
        
#         if current_chunk:
#             chunks.append(current_chunk)
            
#         return chunks



    def load_text(self, text, file_name):
        if file_name in self.uploaded_files:
            return False

        chunks = self.split_text(text)
        for chunk in chunks:
            self.chunks.append(SimpleNamespace(text=chunk, source=file_name))
        self.uploaded_files.append(file_name)
        return True
    
    def load_pdf(self, file_content, file_name):
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(file_content)
                tmp_file_path = tmp_file.name
            
            with open(tmp_file_path, 'rb') as file:
                reader = PdfReader(file)
                for page_num, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    if page_text:
                        chunks = self.split_text(page_text)
                        for chunk in chunks:
                            self.chunks.append(DocumentChunk(
                                text=chunk,
                                doc_name=file_name,
                                page_num=page_num + 1
                            ))
                            self.doc_texts.append(chunk)
                
                if self.chunks:
                    self.uploaded_files.append(file_name)
                    # –û–±–Ω–æ–≤–ª—è–µ–º TF-IDF –º–∞—Ç—Ä–∏—Ü—É
                    self.tfidf_matrix = self.vectorizer.fit_transform(self.doc_texts)
                    return True
                else:
                    st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ {file_name}")
                    return False
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ PDF: {e}")
            return False
        finally:
            if os.path.exists(tmp_file_path):
                os.unlink(tmp_file_path)
    
    def find_most_relevant_chunks(self, query, top_k=3):
        """–ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ —Å –ø–æ–º–æ—â—å—é TF-IDF –∏ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        if not self.chunks:
            return []
            
        query_vec = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vec, self.tfidf_matrix)
        top_indices = np.argsort(similarities[0])[-top_k:][::-1]
        
        return [(self.chunks[i].text, self.chunks[i].doc_name, self.chunks[i].page_num) 
                for i in top_indices if similarities[0][i] > 0.1]
    
    def get_document_names(self):
        return self.uploaded_files

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
if 'knowledge_base' not in st.session_state:
    st.session_state.knowledge_base = KnowledgeBase()

if 'messages' not in st.session_state:
    st.session_state.messages = []

############################################
import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import json

#st.title("–ü–∞—Ä—Å–µ—Ä –≤–∞–∫–∞–Ω—Å–∏–π Moldova Agroindbank —Å rabota.md")

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
}

base_url = "https://www.rabota.md/ru/companies/moldova-agroindbank#vacancies"

if st.button("√éncarcƒÉ ofertele de muncƒÉ de pe rabota.md"):
    try:
        with st.spinner("–ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤–∞–∫–∞–Ω—Å–∏–π..."):
            response = requests.get(base_url, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            links = soup.find_all('a', class_='vacancyShowPopup')
            urls = [urljoin(base_url, a['href']) for a in links]

        st.success(f"–ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(urls)}")

        vacancies_data = []
        progress_bar = st.progress(0)
        status_text = st.empty()

        for i, url in enumerate(urls):
            try:
                response = requests.get(url, headers=headers)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')

                title_tag = soup.find('h1')
                title = title_tag.get_text(strip=True) if title_tag else '–ù–µ –Ω–∞–π–¥–µ–Ω–æ'

                vacancy_content = soup.find('div', class_='vacancy-content')
                description = vacancy_content.get_text(separator='\n', strip=True) if vacancy_content else '–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'

                vacancies_data.append({
                    'url': url,
                    'title': title,
                    'description': description
                })

                status_text.text(f"[{i+1}/{len(urls)}] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—è: {title}")
                progress_bar.progress((i+1)/len(urls))

                time.sleep(0.1)  # –ø–∞—É–∑–∞ –¥–ª—è —É–≤–∞–∂–µ–Ω–∏—è —Å–µ—Ä–≤–µ—Ä–∞

            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {e}")

        st.success("‚úÖ –í—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")

        for vacancy in vacancies_data:
            st.markdown(f'<h4><a href="{vacancy["url"]}" style="color:#40c1ac; text-decoration:none;">{vacancy["title"]}</a></h3>', unsafe_allow_html=True)

        json_data = json.dumps(vacancies_data, ensure_ascii=False, indent=2)
        st.download_button("–°–∫–∞—á–∞—Ç—å –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ JSON", data=json_data, file_name="vacancies.json", mime="application/json")

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤–∞–∫–∞–Ω—Å–∏–π: {e}")


#################################################################



# # –ó–∞–≥—Ä—É–∑–∫–∞ CV
# uploaded_files = st.file_uploader("√éncarcƒÉ CV-ul tƒÉu √Æn format PDF", type="pdf", accept_multiple_files=True)
# if uploaded_files:
#     for uploaded_file in uploaded_files:
#         if uploaded_file.name not in st.session_state.knowledge_base.get_document_names():
#             success = st.session_state.knowledge_base.load_pdf(uploaded_file.getvalue(), uploaded_file.name)
#             if success:
#                 st.success(f"Fi»ôierul {uploaded_file.name} a fost √ÆncƒÉrcat cu succes!")

# if not st.session_state.knowledge_base.get_document_names():
#     st.info("‚ÑπÔ∏è √éncarcƒÉ CV-ul pentru a continua analiza.")
#     st.stop()

# # –ê–Ω–∞–ª–∏–∑ posturilor potrivite
# st.subheader("üîé IdentificƒÉm posturile potrivite pentru tine...")

# # –ü–æ–ª—É—á–∞–µ–º top-3 –≤–∞–∫–∞–Ω—Å–∏–∏, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É CV
# cv_text = "\n\n".join([chunk.text for chunk in st.session_state.knowledge_base.chunks])
# top_k = 3
# vacancy_texts = [f"{v['title']} {v['description']}" for v in vacancies_data]
# vectorizer = TfidfVectorizer(stop_words='english')
# matrix = vectorizer.fit_transform([cv_text] + vacancy_texts)
# similarities = cosine_similarity(matrix[0:1], matrix[1:])[0]
# top_indices = np.argsort(similarities)[-top_k:][::-1]

# # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º top-3 –≤–∞–∫–∞–Ω—Å–∏–∏
# st.subheader("üèÜ Top 3 posturi relevante")
# for i, idx in enumerate(top_indices):
#     vacancy = vacancies_data[idx]
#     st.markdown(f"### {i+1}. [{vacancy['title']}]({vacancy['url']})")
#     st.markdown(vacancy['description'])

# # –ê–Ω–∞–ª–∏–∑ —Å–∞–º–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏
# best_vacancy = vacancies_data[top_indices[0]]
# context = f"CV-ul candidatului:\n{cv_text[:3000]}...\n\nPostul:\n{best_vacancy['title']} - {best_vacancy['description']}"

# prompt_analysis = f"""
# EvalueazƒÉ compatibilitatea dintre CV-ul candidatului »ôi acest post.
# - IdentificƒÉ punctele forte (ce se potrive»ôte bine).
# - Men»õioneazƒÉ ce competen»õe lipsesc sau sunt slabe.
# RƒÉspunde √Æn limba rom√¢nƒÉ.
# """.strip()

# data = {
#     "model": "deepseek-chat",
#     "messages": [
#         {"role": "user", "content": prompt_analysis},
#         {"role": "user", "content": context}
#     ],
#     "max_tokens": 1000,
#     "temperature": 0.2
# }

# st.subheader("üîç Analiza celei mai relevante pozi»õii")
# with st.spinner("Se genereazƒÉ analiza..."):
#     try:
#         response = requests.post(url, headers=headers, json=data)
#         if response.status_code == 200:
#             answer = response.json()['choices'][0]['message']['content']
#             st.markdown(answer + " ‚úÖ")
#         else:
#             st.error(f"‚ùå Eroare API: {response.status_code} - {response.text}")
#     except Exception as e:
#         st.error(f"‚ùå A apƒÉrut o eroare: {e}")
import streamlit as st
from pathlib import Path
import docx2txt
from PyPDF2 import PdfReader
import tempfile
import requests
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# üîπ –ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
class Chunk:
    def __init__(self, text, source):
        self.text = text
        self.source = source

class KnowledgeBase:
    def __init__(self):
        self.chunks = []
        self.uploaded_files = []
        self.vectorizer = TfidfVectorizer(stop_words='english')
        self.tfidf_matrix = None
        self.doc_texts = []

    def split_text(self, text, max_tokens=2000):
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            tokens = tokenizer.tokenize(para)
            if len(tokenizer.tokenize(current_chunk + para)) > max_tokens:
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = para
                else:
                    chunks.append(para)
                    current_chunk = ""
            else:
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para

        if current_chunk:
            chunks.append(current_chunk)

        return chunks

    def _load_docx(self, file_content, file_name):
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp_file:
                tmp_file.write(file_content)
                tmp_file_path = tmp_file.name
            text = docx2txt.process(tmp_file_path)
            self.chunks.append(Chunk(text=text, source=file_name))
            self.doc_names.add(file_name)
            return True
        except Exception as e:
            st.error(f"‚ùå Eroare la citirea DOCX: {e}")
            return False

    def _load_txt(self, file_content, file_name):
        try:
            text = file_content.decode("utf-8")
            self.chunks.append(Chunk(text=text, source=file_name))
            self.doc_names.add(file_name)
            return True
        except Exception as e:
            st.error(f"‚ùå Eroare la citirea TXT: {e}")
            return False

    def get_document_names(self):
        return list(self.doc_names)

# üîπ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
if "knowledge_base" not in st.session_state:
    st.session_state.knowledge_base = KnowledgeBase()

# # üîπ –î–∞–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–π (–ø—Ä–∏–º–µ—Ä)
# vacancies_data = [
#     {"title": "Software Developer", "description": "We are looking for a Python developer with experience in ML.", "url": "https://example.com/dev"},
#     {"title": "Data Analyst", "description": "Candidate should know SQL, Excel and BI tools.", "url": "https://example.com/analyst"},
#     {"title": "DevOps Engineer", "description": "Looking for someone with AWS and CI/CD experience.", "url": "https://example.com/devops"},
#     {"title": "Frontend Developer", "description": "React.js knowledge is a must. Experience with Tailwind is a plus.", "url": "https://example.com/frontend"}
# ]

# üîπ –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∑–∞–≥—Ä—É–∑–∫–∏
# st.title("üìÑ Analizator CV & Potrivire Posturi")
uploaded_files = st.file_uploader(
    "√éncarcƒÉ CV-ul tƒÉu (PDF, DOCX, TXT)", 
    type=["pdf", "docx", "txt"], 
    accept_multiple_files=True
)

import docx2txt
import io

#uploaded_files = st.file_uploader("√éncarcƒÉ CV-ul tƒÉu (PDF, DOCX sau TXT)", type=["pdf", "docx", "txt"], accept_multiple_files=True)

if uploaded_files:
    for uploaded_file in uploaded_files:
        file_name = uploaded_file.name
        if file_name in st.session_state.knowledge_base.uploaded_files:
            continue

        file_bytes = uploaded_file.getvalue()

        if file_name.endswith(".pdf"):
            success = st.session_state.knowledge_base.load_pdf(file_bytes, file_name)

        elif file_name.endswith(".docx"):
            text = docx2txt.process(io.BytesIO(file_bytes))
            success = st.session_state.knowledge_base.load_text(text, file_name)

        elif file_name.endswith(".txt"):
            text = file_bytes.decode("utf-8")
            success = st.session_state.knowledge_base.load_text(text, file_name)

        else:
            st.warning(f"Formatul fi»ôierului {file_name} nu este acceptat.")
            continue

        if success:
            st.success(f"Fi»ôierul {file_name} a fost √ÆncƒÉrcat cu succes!")


if not st.session_state.knowledge_base.get_document_names():
    st.info("‚ÑπÔ∏è √éncarcƒÉ CV-ul pentru a continua analiza.")
    st.stop()

# üîπ –ê–Ω–∞–ª–∏–∑
st.subheader("üîé IdentificƒÉm posturile potrivite pentru tine...")

cv_text = "\n\n".join([chunk.text for chunk in st.session_state.knowledge_base.chunks])
top_k = 3
vacancy_texts = [f"{v['title']} {v['description']}" for v in vacancies_data]

vectorizer = TfidfVectorizer(stop_words='english')
matrix = vectorizer.fit_transform([cv_text] + vacancy_texts)
similarities = cosine_similarity(matrix[0:1], matrix[1:])[0]
top_indices = np.argsort(similarities)[-top_k:][::-1]

# üîπ –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø –≤–∞–∫–∞–Ω—Å–∏–∏
st.subheader("üèÜ Top 3 posturi relevante")
for i, idx in enumerate(top_indices):
    vacancy = vacancies_data[idx]
    st.markdown(f"### {i+1}. [{vacancy['title']}]({vacancy['url']})")
    st.markdown(vacancy['description'])

# üîπ –ê–Ω–∞–ª–∏–∑ –ª—É—á—à–µ–π –≤–∞–∫–∞–Ω—Å–∏–∏
best_vacancy = vacancies_data[top_indices[0]]
context = f"CV-ul candidatului:\n{cv_text[:3000]}...\n\nPostul:\n{best_vacancy['title']} - {best_vacancy['description']}"

prompt_analysis = """
EvalueazƒÉ compatibilitatea dintre CV-ul candidatului »ôi acest post.
- IdentificƒÉ punctele forte (ce se potrive»ôte bine).
- Men»õioneazƒÉ ce competen»õe lipsesc sau sunt slabe.
RƒÉspunde √Æn limba rom√¢nƒÉ.
""".strip()

data = {
    "model": "deepseek-chat",
    "messages": [
        {"role": "user", "content": prompt_analysis},
        {"role": "user", "content": context}
    ],
    "max_tokens": 1000,
    "temperature": 0.2
}

# üîπ –í–Ω–µ—à–Ω–∏–π API (–∑–∞–º–µ–Ω–∏ —Å–≤–æ–∏–º–∏ –∫–ª—é—á–∞–º–∏ –∏ URL)
url = "https://api.your-provider.com/v1/chat/completions"
headers = {"Authorization": f"Bearer YOUR_API_KEY"}

st.subheader("üîç Analiza celei mai relevante pozi»õii")
with st.spinner("Se genereazƒÉ analiza..."):
    try:
        response = requests.post(url, headers=headers, json=data)
        if response.status_code == 200:
            answer = response.json()['choices'][0]['message']['content']
            st.markdown(answer + " ‚úÖ")
        else:
            st.error(f"‚ùå Eroare API: {response.status_code} - {response.text}")
    except Exception as e:
        st.error(f"‚ùå A apƒÉrut o eroare: {e}")

# –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —á–∞—Ç–∞
if st.button("–û—á–∏—Å—Ç–∏—Ç—å —á–∞—Ç"):
    st.session_state.messages = []
    st.rerun()
